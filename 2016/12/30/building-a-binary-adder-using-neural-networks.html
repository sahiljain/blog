<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Building a Binary Adder Using Neural Networks</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://blog.sahiljain.ca/2016/12/30/building-a-binary-adder-using-neural-networks.html">
  <link rel="alternate" type="application/rss+xml" title="Sahil Jain&#39;s Blog" href="https://blog.sahiljain.ca/feed.xml">
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <a class="site-title" href="/">Sahil Jain&#39;s Blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About Me</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Building a Binary Adder Using Neural Networks</h1>
    <p class="post-meta"><time datetime="2016-12-30T00:00:00-08:00" itemprop="datePublished">Dec 30, 2016</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>

<p>While taking Andrew Ng’s <a href="https://www.coursera.org/learn/machine-learning">Machine Learning
course</a>, I came across the idea
that neurons in neural networks can simulate logic gates. I wanted to prove this
by manually building a neural network to perform binary addition, and then later
having a neural network learn binary addition via training. This is a really
interesting concept: it shows that there is an equivalence between digital
circuits and neural networks. The neural network is executing on a digital
computer, and it is able to simulate a digital computer. It’s another paradigm
of computing, and the mapping I am about to describe is sound and complete.
Furthermore, biological neurons perform the same function as artifical neurons,
so it also shows an equivalence between digital computers and the brain.</p>

<p>Let’s first take a look at what an individual neuron does:</p>

<p><img src="/images/neuron.png" alt="Neuron" width="500px" style="margin: 0 auto; display: block;" /></p>

<p>It takes in an array of inputs <script type="math/tex">u_0, u_1, u_2</script>, multiplies each element by its
respective weight, and sums this. If the sum is greater than the threshold
(<script type="math/tex">-b_0</script>), output 1, otherwise output 0. Formally, the output is</p>

<script type="math/tex; mode=display">y_0 = sigmoid(u_0w_0 + u_1w_1 + u_2w_2 + b_0)</script>

<p>where <script type="math/tex">sigmoid</script> is a smoothed step function outputting 1 if the input is <script type="math/tex">>
0</script>. Using this formula, and restricting the inputs to be 0 or 1, we can build
an AND operator:</p>

<p><img src="/images/and.png" alt="Neuron" width="500px" style="margin: 0 auto; display: block;" /></p>

<p>If both inputs are 0, the sum will be -1.5 so the neuron will not activate. If
only 1 input is 1, the sum will be -0.5 so the neuron will not activate. If both
inputs are 1, the sum will be 0.5 so the neuron will activate. Similarly, we can build an OR operator:</p>

<p><img src="/images/or.png" alt="Neuron" width="500px" style="margin: 0 auto; display: block;" /></p>

<p>We can also build a NOT operator:</p>

<p><img src="/images/not.png" alt="Neuron" width="500px" style="margin: 0 auto; display: block;" /></p>

<p>Using these 3 basic logic gates, we can build more complicated gates such as
XOR. We can simplify the XOR gate by realizing that <script type="math/tex">A\ XOR\ B</script> is the same as <script type="math/tex">(A\ OR\ B)\ AND\ NOT\ (A\ AND\ B)</script>:</p>

<p><img src="/images/xor.png" alt="Neuron" width="500px" style="margin: 0 auto; display: block;" /></p>

<p>Now that we have the basic conversions from logic gates to neurons, let’s look
at the logic circuit for a 2-bit binary adder:</p>

<p><img src="/images/adder_circuit.png" alt="Neuron" width="600px" style="margin: 0 auto; display: block;" /></p>

<p>The first output is calculated by XORing the 2 least-significant bits. There is
a carry-over only if the first bits are both 1. Translating each logic gate into
neurons, the neural network for the binary adder should look like:</p>

<p><img src="/images/adder_network.png" alt="Neuron" width="600px" style="margin: 0 auto; display: block;" /></p>

<p>This network takes in 4 inputs (2 bits for input a, 2 bits for input b), and
outputs 3 bits, since adding 2 bit numbers can result in 3 bits (ex. 3 + 3 = 6,
so 11 + 11 = 110). There are 2 hidden layers: the first hidden layer has 6
neurons, and the second hidden layer has 4 neurons. Going from left to right,
the layers are of size: 4,6,4,5,3.</p>

<p>We can manually test this network we have built on some inputs and see that it
works. Now, let’s use Tensorflow and automatically train a neural network of the
same shape to learn binary addition!</p>

<p>We’ll start by importing Tensorflow, and setting the size of the input layer,
the 3 intermediate layers, and the output layer:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="n">input_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">hidden_1_size</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">hidden_2_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">hidden_3_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">3</span></code></pre></figure>

<p>The Tensorflow python library is mostly just an interface to the main C++
backend which runs in a separate process. In order to communicate with the C++
backend, we need to specify shared data in the form of tf Variables. The input
(the 2 2-bit numbers) will be one variable (a vector of size 4), the output will be another (a vector of size 3), and the
weights and biases of the network will also be variables. We will specify the
weights as matrices.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Variable to populate with input</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s">"float"</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">input_size</span><span class="p">])</span>

<span class="c"># Variable which will be populated with output</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s">"float"</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">output_size</span><span class="p">])</span>

<span class="c"># Variables which we will be learning</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'h1'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_1_size</span><span class="p">])),</span>
  <span class="s">'h2'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">hidden_1_size</span><span class="p">,</span> <span class="n">hidden_2_size</span><span class="p">])),</span>
  <span class="s">'h3'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">hidden_2_size</span><span class="p">,</span> <span class="n">hidden_3_size</span><span class="p">])),</span>
  <span class="s">'out'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">hidden_3_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">]))</span>
<span class="p">}</span>

<span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'b1'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">hidden_1_size</span><span class="p">])),</span>
  <span class="s">'b2'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">hidden_2_size</span><span class="p">])),</span>
  <span class="s">'b3'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">hidden_3_size</span><span class="p">])),</span>
  <span class="s">'out'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">output_size</span><span class="p">]))</span>
<span class="p">}</span></code></pre></figure>

<p>The rest of the Tensorflow program is left as an exercise to the reader.</p>

<div>
  <h1 id="comments">Comments</h1>
</div>
<div id="disqus_thread"></div>
<script>
  var disqus_config = function () {
    this.page.url = "http://blog.sahiljain.ca/2016/12/30/building-a-binary-adder-using-neural-networks.html";
    this.page.identifier = "/2016/12/30/building-a-binary-adder-using-neural-networks";
  };
  (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//sahiljainblog.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>


  </div>

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Sahil Jain&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Sahil Jain&#39;s Blog</li>
          <li><a href="mailto:sahil.jain@uwaterloo.ca">sahil.jain@uwaterloo.ca</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/sahiljain"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">sahiljain</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/_sahil_jain"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">_sahil_jain</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>I&#39;m Sahil, a Software Engineering student at the University of Waterloo.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
